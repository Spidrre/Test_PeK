{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "path_to_imgs = 'sample_images/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_saturation(image, target_saturation):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV).astype(np.float32)\n",
    "    mean_saturation = hsv[:, :, 1].mean() / 255.0\n",
    "    adjustment_ratio = target_saturation / mean_saturation\n",
    "    hsv[:, :, 1] = np.clip(hsv[:, :, 1] * adjustment_ratio, 0, 255)\n",
    "    return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_images(model: YOLO, image_folder: str, confidence: float=0.5, target_saturation: float=0.42) -> None:\n",
    "    path_to_imgs = image_folder+'/images'\n",
    "    names = model.names\n",
    "\n",
    "    for file in os.listdir(path_to_imgs):\n",
    "\n",
    "        if not file.lower().endswith(('.jpg', '.png', '.bmp', 'jpeg')):\n",
    "            continue\n",
    "        \n",
    "        list_of_centers = []\n",
    "        image_path = os.path.join(path_to_imgs, file)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = adjust_saturation(image, target_saturation)\n",
    "\n",
    "        height, width, _ = image.shape\n",
    "\n",
    "        results = model.predict(image, conf=confidence)[0]\n",
    "\n",
    "        # Создаем копию изображения для рисования\n",
    "        image_copy = image.copy()\n",
    "\n",
    "        # Получаем все детекции\n",
    "        detections = results.boxes.xyxy.cpu().numpy()\n",
    "        classes = results.boxes.cls.cpu().numpy()\n",
    "        \n",
    "\n",
    "        # Цикл по всем детекциям\n",
    "        for i, detection in enumerate(detections):\n",
    "            predict_cls = names[classes[i]]\n",
    "            if predict_cls.lower() != 'apple':\n",
    "                continue\n",
    "            # Получаем координаты центра bounding box\n",
    "            x_center = int((detection[0] + detection[2]) / 2)\n",
    "            y_center = int((detection[1] + detection[3]) / 2)\n",
    "            list_of_centers.append((x_center, y_center))\n",
    "            \n",
    "            # Рисуем центр bounding box на изображении\n",
    "            cv2.circle(image_copy, (x_center, y_center), 25, (255, 0, 0), -1)\n",
    "\n",
    "        \n",
    "\n",
    "        formatted_rel_list_of_centers = [ '%.2f' % (elem[0]/width) + ', ' + '%.2f' % (elem[1]/height) for elem in list_of_centers ]\n",
    "        print(f'List of centers for {file}\\nAbsolute values {list_of_centers}\\nRelative values {formatted_rel_list_of_centers}')\n",
    "\n",
    "        # Проверяем, соответствуют ли предсказания меткам\n",
    "        label_path = os.path.join(image_folder+'/labels', os.path.basename(file)[0]+'.txt')\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                \n",
    "                labels = f.readlines()\n",
    "                true_predictions = 0\n",
    "\n",
    "                \n",
    "                \n",
    "                for center in list_of_centers:\n",
    "                    \n",
    "                    closest_dist = float(\"inf\")\n",
    "                    closest_box = None  \n",
    "                    for i, label in enumerate(labels):\n",
    "                        _, x, y, w, h = map(float, label.split())\n",
    "                        x_center_label = int(x * width)\n",
    "                        y_center_label = int(y * height)\n",
    "                        x_left_label = int((x-w)*width) if x-w >= 0 else 0\n",
    "                        x_right_label = int((x+w)*width) if x+w <= width else width\n",
    "                        y_top_label = int((y-h)*height) if y-h >= 0 else 0\n",
    "                        y_down_label = int((y+h)*height) if y+h <= height else height\n",
    "                        cv2.circle(image_copy, (x_center_label, y_center_label), 25, (0, 0, 255), -1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        if (\n",
    "                                np.sqrt((center[0] - x_center_label)**2 + (center[1] - y_center_label)**2) < closest_dist and\n",
    "                                x_left_label <= center[0] <= x_right_label and\n",
    "                                y_top_label <= center[1] <= y_down_label\n",
    "                            ):\n",
    "                            \n",
    "                            closest_dist = np.sqrt((center[0] - x_center_label)**2 + (center[1] - y_center_label)**2)\n",
    "                            closest_box = i\n",
    "\n",
    "                    if closest_box is not None:\n",
    "                        true_predictions +=1\n",
    "                \n",
    "\n",
    "                print(f'True predictions for {file}: {true_predictions} out of {len(labels)}, accuracy: {true_predictions/len(labels):.2f}')\n",
    "        else:\n",
    "            print(f'No label file found for {file}')\n",
    "\n",
    "\n",
    "        # Отображаем измененное изображение\n",
    "        image_copy = cv2.resize(image_copy, (640, 448))\n",
    "        plt.imshow(cv2.cvtColor(image_copy, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Image with centers')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.68  Python-3.12.2 torch-2.4.0+cu124 CUDA:0 (NVIDIA GeForce RTX 4070, 12282MiB)\n",
      "Model summary (fused): 168 layers, 11,125,971 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning F:\\PycharmProjects\\randoms\\test_PeK\\sample_images\\labels.cache... 6 images, 0 backgrounds, 0 corrupt: 100%|██████████| 6/6 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          6        101      0.854      0.347      0.616      0.273\n",
      "Speed: 7.0ms preprocess, 30.6ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('apple_detection/train3/weights/best.pt')\n",
    "validation_results = model.val(data=\"sample_images/data.yaml\", imgsz=640, conf=0.5, device=\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average saturation across all images in the folder: 0.486873031231777\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def average_saturation(folder_path):\n",
    "    \"\"\"\n",
    "    Calculates the average saturation for all images in the specified folder.\n",
    "    Parameters:\n",
    "    folder_path (str): The path to the folder containing the images.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    total_saturation = 0 \n",
    "    num_images = 0 \n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            img = cv2.imread(os.path.join(folder_path, filename)) \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) \n",
    "            saturation = np.mean(img[:,:,1])/255 \n",
    "            total_saturation += saturation  \n",
    "            num_images += 1  \n",
    "    \n",
    "    if num_images > 0:\n",
    "        average_saturation = total_saturation / num_images  \n",
    "        print(f'Average saturation across all images in the folder: {average_saturation}')  \n",
    "    else:\n",
    "        print('No images found in the folder')\n",
    "\n",
    "average_saturation('This-Apple-1/train/images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'\u001b[31m\u001b[1muse_sahi\u001b[0m' is not a valid YOLO argument. Similar arguments are i.e. ['close_mosaic=10'].\n\n    Arguments received: ['yolo', '--f=c:\\\\Users\\\\Spidrre\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v391d19e0a2867ea9414581916ef55d8a2252f6feb.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of {'pose', 'segment', 'obb', 'detect', 'classify'}\n                MODE (required) is one of {'track', 'train', 'export', 'val', 'predict', 'benchmark'}\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco8.yaml model=yolov8n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolov8n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolov8n.pt data=coco8.yaml batch=1 imgsz=640\n\n    4. Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n\n    5. Explore your datasets using semantic search and SQL with a simple GUI powered by Ultralytics Explorer API\n        yolo explorer data=data.yaml model=yolov8n.pt\n    \n    6. Streamlit real-time object detection on your webcam with Ultralytics YOLOv8\n        yolo streamlit-predict\n        \n    7. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n\n    Docs: https://docs.ultralytics.com\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n     (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\Spidrre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[5], line 3\u001b[0m\n    run_images(model, path_to_imgs, target_saturation=0.486873031231777)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[3], line 20\u001b[0m in \u001b[0;35mrun_images\u001b[0m\n    results = model.predict(image, conf=confidence, use_sahi=True)[0]\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\Spidrre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\model.py:555\u001b[0m in \u001b[0;35mpredict\u001b[0m\n    self.predictor = predictor or self._smart_load(\"predictor\")(overrides=args, _callbacks=self.callbacks)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\Spidrre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:88\u001b[0m in \u001b[0;35m__init__\u001b[0m\n    self.args = get_cfg(cfg, overrides)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\Spidrre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\cfg\\__init__.py:253\u001b[0m in \u001b[0;35mget_cfg\u001b[0m\n    check_dict_alignment(cfg, overrides)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32mc:\\Users\\Spidrre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\cfg\\__init__.py:440\u001b[1;36m in \u001b[1;35mcheck_dict_alignment\u001b[1;36m\n\u001b[1;33m    raise SyntaxError(string + CLI_HELP_MSG) from e\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m<string>\u001b[1;36m\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m '\u001b[31m\u001b[1muse_sahi\u001b[0m' is not a valid YOLO argument. Similar arguments are i.e. ['close_mosaic=10'].\n\n    Arguments received: ['yolo', '--f=c:\\\\Users\\\\Spidrre\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v391d19e0a2867ea9414581916ef55d8a2252f6feb.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of {'pose', 'segment', 'obb', 'detect', 'classify'}\n                MODE (required) is one of {'track', 'train', 'export', 'val', 'predict', 'benchmark'}\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco8.yaml model=yolov8n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolov8n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolov8n.pt data=coco8.yaml batch=1 imgsz=640\n\n    4. Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n\n    5. Explore your datasets using semantic search and SQL with a simple GUI powered by Ultralytics Explorer API\n        yolo explorer data=data.yaml model=yolov8n.pt\n    \n    6. Streamlit real-time object detection on your webcam with Ultralytics YOLOv8\n        yolo streamlit-predict\n        \n    7. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n\n    Docs: https://docs.ultralytics.com\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n    \n"
     ]
    }
   ],
   "source": [
    "model = YOLO('apple_detection/train4/weights/best.pt')\n",
    "run_images(model, path_to_imgs, target_saturation=0.486873031231777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing prediction on 24 slices.\n",
      "Performing prediction on 24 slices.\n",
      "Performing prediction on 24 slices.\n",
      "\n",
      "0: 384x640 5 apples, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 apples, 6.9ms\n",
      "Speed: 2.1ms preprocess, 6.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 apples, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# Загрузка модели\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_prediction, get_sliced_prediction\n",
    "import math\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "#model_path = 'apple_detection/100m3/weights/best.pt'\n",
    "model_path = 'yolov8m.pt'\n",
    "\n",
    "\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type=\"yolov8\",\n",
    "    model_path=model_path,\n",
    "    #model_path='yolov8m.pt',\n",
    "    confidence_threshold=0.7,\n",
    "    device=\"cuda:0\",  # or 'cuda:0'\n",
    ")\n",
    "\n",
    "model = YOLO(model_path)\n",
    "\n",
    "path_to_imgs = 'sample_images/images'\n",
    "for file in os.listdir(path_to_imgs):\n",
    "\n",
    "        if not file.lower().endswith(('.jpg', '.png', '.bmp', 'jpeg')):\n",
    "            continue\n",
    "        \n",
    "        list_of_centers = []\n",
    "        image_path = os.path.join(path_to_imgs, file)\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        img_height, img_width, _ = image.shape\n",
    "\n",
    "        if img_height > 2000 or img_width > 2000:\n",
    "\n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            window_size = 1024\n",
    "\n",
    "            height_extra = math.floor((math.ceil(img_height / window_size) * window_size - img_height) / math.ceil(img_height / window_size)) / window_size\n",
    "            width_extra = math.floor((math.ceil(img_width / window_size) * window_size - img_width) / math.ceil(img_width / window_size)) / window_size\n",
    "            y_overlap = height_extra\n",
    "            x_overlap = width_extra\n",
    "\n",
    "            result = get_sliced_prediction(\n",
    "                image,\n",
    "                detection_model,\n",
    "                slice_height=window_size,\n",
    "                slice_width=window_size,\n",
    "                overlap_height_ratio=y_overlap,\n",
    "                overlap_width_ratio=x_overlap,\n",
    "                postprocess_type='NMS',\n",
    "                postprocess_match_metric='IOU',\n",
    "                postprocess_match_threshold=0.2,\n",
    "            )\n",
    "\n",
    "            result.export_visuals(export_dir=f\"demo/test\", file_name=f\"{file}_result.png\")\n",
    "\n",
    "\n",
    "        else:\n",
    "             #image = adjust_saturation(image, 0.486873031231777)\n",
    "             results = model(image, conf=0.4, iou=0.7)\n",
    "             # Сохранить результаты извлечения модели yolov8 для обнаружения объектов как изображение\n",
    "             for result in results:\n",
    "                result.save(f\"demo/test/{file}_result.png\")\n",
    "             \n",
    "             "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
